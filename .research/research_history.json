{
  "research_topic": "Improved Chain-of-Thought prompting aligned with human thinking",
  "research_hypothesis": {
    "open_problems": "Current Chain-of-Thought prompting lacks explicit alignment with human cognitive steps, leading to suboptimal reasoning on multi-step tasks.",
    "method": "Introduce a structured prompt template that mirrors human thinking phases (problem decomposition, analogical reasoning, verification) without any model fine-tuning.",
    "experimental_setup": "Evaluate on GSM8K and ARC-Challenge benchmarks using GPT-4o with the proposed prompt template vs. standard CoT baseline.",
    "primary_metric": "Accuracy (%) on GSM8K and ARC-Challenge test sets.",
    "experimental_code": "Run evaluation scripts using the OpenAI API with temperature=0 for reproducibility.",
    "expected_result": "The proposed human-aligned CoT template achieves at least 3% higher accuracy than standard CoT on both benchmarks.",
    "expected_conclusion": "Structuring prompts to reflect human cognitive phases improves LLM reasoning without requiring fine-tuning."
  },
  "experimental_design": {
    "experiment_summary": "Task: have an LLM solve multi-step reasoning problems and produce a single final answer (a number for GSM8K; a multiple-choice option A/B/C/D for ARC-Challenge), then score correctness against gold labels.\n\nPurpose: test whether a human-aligned, phase-structured prompting template (Decompose → Analogize → Verify → Final) improves zero-shot reasoning accuracy compared with a standard Chain-of-Thought (CoT) prompt, without any fine-tuning.\n\nWorkflow:\n1) Load GSM8K test (1319 problems) and ARC-Challenge test (1172 questions).\n2) For each example, query the same base model (temperature=0, max_tokens capped) under two prompting conditions:\n   - Baseline: standard CoT prompt.\n   - Proposed: structured “human cognitive phases” prompt template (decomposition, analogical reasoning, verification).\n3) Parse the model’s final answer into a normalized form (numeric for GSM8K; option letter for ARC).\n4) Compute accuracy per benchmark and the combined macro average; run paired significance tests via bootstrap over items.\n5) Report accuracy deltas and error breakdowns (wrong answer vs. unparseable/format violations).\n\nScale fit to Runner (self-hosted, gpu-runner; H200 140GB VRAM, 240GB RAM): the experiment is inference-only via API and lightweight CPU/RAM usage. To ensure it fits time/throughput constraints, use:\n- Deterministic decoding (temperature=0) and capped outputs (e.g., max_tokens=512) to bound latency/cost.\n- Asynchronous batching with conservative concurrency (e.g., 16–64 in-flight requests depending on provider limits).\n- Local caching of API responses and checkpointing every N examples to allow resume.\n- Optional “quick check” mode (e.g., first 200 items per dataset) for debugging before full test-set runs.",
    "runner_config": {
      "runner_label": [
        "self-hosted",
        "gpu-runner"
      ],
      "description": "NVIDIA H200, VRAM: 140 GB, RAM: 240 GB"
    },
    "evaluation_metrics": [
      {
        "name": "Accuracy (%) on GSM8K and ARC-Challenge test sets.",
        "description": "Correctness criteria:\n- GSM8K: A prediction is correct if the extracted final numeric answer matches the gold numeric answer exactly after normalization. Normalization: strip commas/whitespace, allow leading ‘$’, convert to Decimal if possible; treat integers and decimals as exact string-equal after canonicalization (e.g., ‘12.0’ → ‘12’). If the gold is an integer and the model outputs an equivalent integer-form number, mark correct. If parsing fails, mark incorrect.\n- ARC-Challenge: A prediction is correct if the extracted final option letter ∈ {A,B,C,D} matches the gold answerKey (case-insensitive). If multiple letters are produced, use the last explicitly stated final choice; if none parse, mark incorrect.\n\nCalculation method:\nFor each dataset d ∈ {GSM8K, ARC-Challenge}:\n  Accuracy_d(%) = 100 * (1/N_d) * Σ_i 1[pred_i == gold_i]\nReport both per-dataset accuracies and the macro-average across the two datasets:\n  MacroAcc(%) = (Accuracy_GSM8K + Accuracy_ARC)/2\n\nTask appropriateness:\nBoth benchmarks are discrete-answer reasoning tasks where success is defined by reaching the correct final answer, making accuracy the most direct and comparable metric across prompting methods.\n\nRelevant visualizations:\n- Bar chart of accuracy per method per dataset with 95% bootstrap confidence intervals.\n- Paired delta plot (proposed − baseline) with bootstrap CI for each dataset."
      },
      {
        "name": "Unparseable/Format Violation Rate (%)",
        "description": "Correctness criteria:\n- Count a response as unparseable if the evaluator cannot extract a final numeric answer (GSM8K) or a valid option letter A–D (ARC-Challenge) from the model output using the specified parsing rules.\n\nCalculation method:\nFor each dataset d:\n  UnparseableRate_d(%) = 100 * (1/N_d) * Σ_i 1[parse_failed_i]\n\nTask appropriateness:\nPrompt templates can change verbosity/formatting; separating parsing/format failures from genuine reasoning errors helps interpret accuracy differences and ensures fairness.\n\nRelevant visualizations:\n- Stacked bar chart: {correct, wrong, unparseable} proportions by method and dataset."
      },
      {
        "name": "Bootstrap Paired Significance (p-value)",
        "description": "Correctness criteria:\n- Use paired comparison because the same items are evaluated under both prompts.\n\nCalculation method:\n- For each dataset, compute per-item correctness difference Δ_i = correct_proposed_i − correct_baseline_i.\n- Bootstrap B=10,000 resamples of items with replacement; compute mean(Δ) for each resample.\n- Two-sided p-value estimated as 2 * min(P(mean(Δ) ≤ 0), P(mean(Δ) ≥ 0)). Also report 95% percentile CI for mean(Δ) and translate to percentage-point accuracy delta.\n\nTask appropriateness:\nAccuracy differences of ~3% can be sensitive to sample size; bootstrap CIs/p-values quantify whether improvements are robust rather than noise.\n\nRelevant visualizations:\n- Histogram/density plot of bootstrap mean(Δ) with zero line and 95% CI markers."
      }
    ],
    "models_to_use": [
      "GPT-4o (params: undisclosed; OpenAI API)"
    ],
    "datasets_to_use": [
      "GSM8K + ARC-Challenge (HF sources: openai/gsm8k and allenai/ai2_arc; evaluated on official test splits)"
    ],
    "proposed_method": {
      "method_name": "Human-Aligned Phased CoT Prompting (HAP-CoT)",
      "description": "Inference-only prompting template that explicitly structures reasoning into human-like phases: (1) Problem Decomposition (identify givens/unknowns/subtasks), (2) Analogical Reasoning (map to known patterns/operations), (3) Verification (check units, constraints, recompute key steps), followed by (4) Final Answer in a strict format. No model fine-tuning; only prompt structure changes.",
      "optuna_config": {
        "enabled": false,
        "n_trials": 20
      }
    },
    "comparative_methods": [
      {
        "method_name": "Standard Chain-of-Thought Prompting (CoT-Baseline)",
        "description": "Conventional zero-shot Chain-of-Thought prompt (e.g., ‘Let’s think step by step.’) without explicit decomposition/analogy/verification phases; same model and decoding settings.",
        "optuna_config": {
          "enabled": false,
          "n_trials": 20
        }
      }
    ]
  },
  "experiment_code": {
    "files": {
      "src/evaluate.py": "\"\"\"Evaluation script for aggregating results from WandB and generating comparison plots.\"\"\"\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport wandb\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.use('Agg')  # Use non-interactive backend\nimport pandas as pd\nimport numpy as np\n\n\ndef fetch_run_from_wandb(entity: str, project: str, run_id: str) -> Dict[str, Any]:\n    \"\"\"Fetch run data from WandB by display name.\n    \n    Args:\n        entity: WandB entity\n        project: WandB project\n        run_id: Run display name\n        \n    Returns:\n        Dictionary with run data (config, summary, history)\n    \"\"\"\n    api = wandb.Api()\n    \n    # Query runs by display name\n    runs = api.runs(\n        f\"{entity}/{project}\",\n        filters={\"display_name\": run_id},\n        order=\"-created_at\"\n    )\n    \n    if not runs:\n        raise ValueError(f\"No run found with display name: {run_id}\")\n    \n    # Get most recent run with this name\n    run = runs[0]\n    \n    # Fetch history (time series data)\n    history = run.history()\n    \n    return {\n        \"config\": dict(run.config),\n        \"summary\": dict(run.summary),\n        \"history\": history,\n        \"url\": run.url,\n        \"id\": run.id\n    }\n\n\ndef export_per_run_metrics(\n    results_dir: Path,\n    run_id: str,\n    run_data: Dict[str, Any]\n):\n    \"\"\"Export per-run metrics and create figures.\n    \n    Args:\n        results_dir: Base results directory\n        run_id: Run identifier\n        run_data: Run data from WandB\n    \"\"\"\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Export metrics\n    metrics = {\n        \"run_id\": run_id,\n        \"accuracy\": run_data[\"summary\"].get(\"final_accuracy\", run_data[\"summary\"].get(\"accuracy\")),\n        \"correct\": run_data[\"summary\"].get(\"correct\"),\n        \"total\": run_data[\"summary\"].get(\"total\"),\n        \"unparseable\": run_data[\"summary\"].get(\"unparseable\"),\n        \"samples_processed\": run_data[\"summary\"].get(\"samples_processed\"),\n        \"wandb_url\": run_data[\"url\"]\n    }\n    \n    with open(run_dir / \"metrics.json\", \"w\") as f:\n        json.dump(metrics, f, indent=2)\n    \n    print(f\"Exported metrics for {run_id}: {run_dir / 'metrics.json'}\")\n    \n    # Create per-run figure (bar chart of metrics)\n    fig, ax = plt.subplots(figsize=(8, 6))\n    \n    metric_names = [\"Correct\", \"Incorrect\", \"Unparseable\"]\n    correct = metrics.get(\"correct\", 0)\n    total = metrics.get(\"total\", 0)\n    unparseable = metrics.get(\"unparseable\", 0)\n    incorrect = total - correct\n    \n    values = [correct, incorrect, unparseable]\n    colors = [\"green\", \"red\", \"orange\"]\n    \n    ax.bar(metric_names, values, color=colors, alpha=0.7)\n    ax.set_ylabel(\"Count\")\n    ax.set_title(f\"{run_id}\\nAccuracy: {metrics['accuracy']:.4f}\")\n    ax.grid(axis='y', alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(run_dir / f\"{run_id}_breakdown.pdf\", format='pdf', dpi=300)\n    plt.close()\n    \n    print(f\"Created figure: {run_dir / f'{run_id}_breakdown.pdf'}\")\n\n\ndef create_comparison_plots(\n    results_dir: Path,\n    run_ids: List[str],\n    all_run_data: Dict[str, Dict[str, Any]]\n):\n    \"\"\"Create comparison plots across runs.\n    \n    Args:\n        results_dir: Base results directory\n        run_ids: List of run IDs to compare\n        all_run_data: Dictionary of run data by run_id\n    \"\"\"\n    comparison_dir = results_dir / \"comparison\"\n    comparison_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract metrics for comparison\n    metrics_by_run = {}\n    for run_id in run_ids:\n        data = all_run_data[run_id]\n        metrics_by_run[run_id] = {\n            \"accuracy\": data[\"summary\"].get(\"final_accuracy\", data[\"summary\"].get(\"accuracy\", 0)),\n            \"correct\": data[\"summary\"].get(\"correct\", 0),\n            \"total\": data[\"summary\"].get(\"total\", 0),\n            \"unparseable\": data[\"summary\"].get(\"unparseable\", 0)\n        }\n    \n    # Create accuracy comparison bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    accuracies = [metrics_by_run[rid][\"accuracy\"] for rid in run_ids]\n    colors = [\"blue\" if \"proposed\" in rid else \"gray\" for rid in run_ids]\n    \n    bars = ax.bar(range(len(run_ids)), accuracies, color=colors, alpha=0.7)\n    ax.set_xticks(range(len(run_ids)))\n    ax.set_xticklabels(run_ids, rotation=45, ha='right')\n    ax.set_ylabel(\"Accuracy\")\n    ax.set_title(\"Accuracy Comparison Across Runs\")\n    ax.set_ylim(0, 1.0)\n    ax.grid(axis='y', alpha=0.3)\n    \n    # Add value labels on bars\n    for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{acc:.3f}',\n                ha='center', va='bottom', fontsize=9)\n    \n    plt.tight_layout()\n    plt.savefig(comparison_dir / \"comparison_accuracy.pdf\", format='pdf', dpi=300)\n    plt.close()\n    \n    print(f\"Created comparison plot: {comparison_dir / 'comparison_accuracy.pdf'}\")\n    \n    # Create grouped bar chart for all metrics\n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    x = np.arange(len(run_ids))\n    width = 0.25\n    \n    correct_counts = [metrics_by_run[rid][\"correct\"] for rid in run_ids]\n    incorrect_counts = [metrics_by_run[rid][\"total\"] - metrics_by_run[rid][\"correct\"] for rid in run_ids]\n    unparseable_counts = [metrics_by_run[rid][\"unparseable\"] for rid in run_ids]\n    \n    ax.bar(x - width, correct_counts, width, label='Correct', color='green', alpha=0.7)\n    ax.bar(x, incorrect_counts, width, label='Incorrect', color='red', alpha=0.7)\n    ax.bar(x + width, unparseable_counts, width, label='Unparseable', color='orange', alpha=0.7)\n    \n    ax.set_xlabel('Run ID')\n    ax.set_ylabel('Count')\n    ax.set_title('Detailed Comparison Across Runs')\n    ax.set_xticks(x)\n    ax.set_xticklabels(run_ids, rotation=45, ha='right')\n    ax.legend()\n    ax.grid(axis='y', alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(comparison_dir / \"comparison_detailed.pdf\", format='pdf', dpi=300)\n    plt.close()\n    \n    print(f\"Created comparison plot: {comparison_dir / 'comparison_detailed.pdf'}\")\n    \n    # Export aggregated metrics\n    proposed_accuracies = [\n        metrics_by_run[rid][\"accuracy\"] \n        for rid in run_ids if \"proposed\" in rid\n    ]\n    baseline_accuracies = [\n        metrics_by_run[rid][\"accuracy\"] \n        for rid in run_ids if \"comparative\" in rid\n    ]\n    \n    best_proposed = max(proposed_accuracies) if proposed_accuracies else 0\n    best_baseline = max(baseline_accuracies) if baseline_accuracies else 0\n    gap = best_proposed - best_baseline\n    \n    aggregated = {\n        \"primary_metric\": \"accuracy\",\n        \"metrics_by_run\": metrics_by_run,\n        \"best_proposed\": best_proposed,\n        \"best_baseline\": best_baseline,\n        \"gap\": gap,\n        \"proposed_runs\": [rid for rid in run_ids if \"proposed\" in rid],\n        \"baseline_runs\": [rid for rid in run_ids if \"comparative\" in rid]\n    }\n    \n    with open(comparison_dir / \"aggregated_metrics.json\", \"w\") as f:\n        json.dump(aggregated, f, indent=2)\n    \n    print(f\"Exported aggregated metrics: {comparison_dir / 'aggregated_metrics.json'}\")\n    print(f\"\\nSummary:\")\n    print(f\"  Best proposed: {best_proposed:.4f}\")\n    print(f\"  Best baseline: {best_baseline:.4f}\")\n    print(f\"  Gap: {gap:.4f}\")\n\n\ndef main():\n    \"\"\"Main evaluation script.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Evaluate and compare runs from WandB\")\n    parser.add_argument(\"--results_dir\", type=str, required=True, help=\"Results directory\")\n    parser.add_argument(\"--run_ids\", type=str, required=True, help=\"JSON list of run IDs\")\n    parser.add_argument(\"--entity\", type=str, default=None, help=\"WandB entity (optional)\")\n    parser.add_argument(\"--project\", type=str, default=None, help=\"WandB project (optional)\")\n    \n    args = parser.parse_args()\n    \n    # Parse run_ids from JSON string\n    run_ids = json.loads(args.run_ids)\n    \n    # Get WandB config from args or environment\n    entity = args.entity or os.getenv(\"WANDB_ENTITY\", \"airas\")\n    project = args.project or os.getenv(\"WANDB_PROJECT\", \"2026-0223-hypothesis\")\n    \n    print(f\"Fetching results from WandB: {entity}/{project}\")\n    print(f\"Run IDs: {run_ids}\")\n    \n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Fetch data for all runs\n    all_run_data = {}\n    for run_id in run_ids:\n        print(f\"\\nFetching data for: {run_id}\")\n        try:\n            run_data = fetch_run_from_wandb(entity, project, run_id)\n            all_run_data[run_id] = run_data\n            print(f\"  Accuracy: {run_data['summary'].get('final_accuracy', run_data['summary'].get('accuracy', 'N/A'))}\")\n        except Exception as e:\n            print(f\"  Error: {e}\")\n            continue\n    \n    if not all_run_data:\n        print(\"No run data fetched. Exiting.\")\n        return\n    \n    # Export per-run metrics and figures\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Exporting per-run metrics and figures...\")\n    print(\"=\" * 80)\n    for run_id, run_data in all_run_data.items():\n        export_per_run_metrics(results_dir, run_id, run_data)\n    \n    # Create comparison plots\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Creating comparison plots...\")\n    print(\"=\" * 80)\n    create_comparison_plots(results_dir, list(all_run_data.keys()), all_run_data)\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"Evaluation complete!\")\n    print(\"=\" * 80)\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "src/inference.py": "\"\"\"Inference script for running LLM evaluations.\"\"\"\n\nimport asyncio\nimport json\nimport os\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional\nfrom omegaconf import DictConfig, OmegaConf\nimport wandb\nfrom openai import AsyncOpenAI\nfrom tqdm import tqdm\nimport sys\n\nfrom src.preprocess import get_dataset\n\n\ndef parse_numeric_answer(text: str) -> Optional[str]:\n    \"\"\"Extract numeric answer from model output.\n    \n    Looks for patterns like \"The answer is: 123\" or \"#### 123\"\n    \"\"\"\n    # Look for \"The answer is: NUMBER\"\n    match = re.search(r\"[Tt]he answer is:?\\s*(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)\", text)\n    if match:\n        # Remove commas from number\n        return match.group(1).replace(\",\", \"\")\n    \n    # Look for \"#### NUMBER\" pattern\n    match = re.search(r\"####\\s*(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)\", text)\n    if match:\n        return match.group(1).replace(\",\", \"\")\n    \n    # Look for any number at the end\n    match = re.search(r\"(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*$\", text.strip())\n    if match:\n        return match.group(1).replace(\",\", \"\")\n    \n    return None\n\n\ndef parse_multiple_choice_answer(text: str) -> Optional[str]:\n    \"\"\"Extract multiple choice answer (A/B/C/D) from model output.\"\"\"\n    # Look for \"The answer is: A\"\n    match = re.search(r\"[Tt]he answer is:?\\s*([A-D])\", text)\n    if match:\n        return match.group(1)\n    \n    # Look for standalone letter at the end\n    match = re.search(r\"\\b([A-D])\\b\\s*$\", text.strip())\n    if match:\n        return match.group(1)\n    \n    # Look for any occurrence of A/B/C/D near the end\n    matches = re.findall(r\"\\b([A-D])\\b\", text[-100:])\n    if matches:\n        return matches[-1]\n    \n    return None\n\n\nasync def query_openai_async(\n    client: AsyncOpenAI,\n    prompt: str,\n    model: str,\n    temperature: float,\n    max_tokens: int,\n    semaphore: asyncio.Semaphore\n) -> Dict[str, Any]:\n    \"\"\"Query OpenAI API asynchronously with rate limiting.\"\"\"\n    async with semaphore:\n        try:\n            response = await client.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=temperature,\n                max_tokens=max_tokens\n            )\n            return {\n                \"response\": response.choices[0].message.content,\n                \"usage\": {\n                    \"prompt_tokens\": response.usage.prompt_tokens,\n                    \"completion_tokens\": response.usage.completion_tokens,\n                    \"total_tokens\": response.usage.total_tokens\n                }\n            }\n        except Exception as e:\n            return {\n                \"response\": None,\n                \"error\": str(e)\n            }\n\n\nasync def run_inference_async(cfg: DictConfig):\n    \"\"\"Run inference on dataset using LLM API.\"\"\"\n    # Load dataset\n    print(f\"Loading dataset: {cfg.run.dataset.name}\")\n    dataset = get_dataset(\n        name=cfg.run.dataset.name,\n        split=cfg.run.dataset.split,\n        cache_dir=cfg.run.dataset.cache_dir\n    )\n    \n    # Limit dataset size in sanity_check mode\n    if cfg.mode == \"sanity_check\":\n        dataset = dataset[:10]\n        print(f\"Sanity check mode: using {len(dataset)} samples\")\n    \n    print(f\"Dataset loaded: {len(dataset)} examples\")\n    \n    # Initialize OpenAI client\n    client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n    \n    # Create semaphore for rate limiting\n    max_concurrent = cfg.run.inference.max_concurrent\n    if cfg.mode == \"sanity_check\":\n        max_concurrent = min(max_concurrent, 8)\n    semaphore = asyncio.Semaphore(max_concurrent)\n    \n    # Create prompts\n    prompt_template = cfg.run.method.prompt_template\n    prompts = []\n    for example in dataset:\n        if cfg.run.dataset.name == \"gsm8k\":\n            prompt = prompt_template.format(question=example[\"question\"])\n        elif cfg.run.dataset.name == \"arc-challenge\":\n            prompt = prompt_template.format(\n                question=example[\"question\"],\n                choices=example[\"choices\"]\n            )\n        else:\n            raise ValueError(f\"Unknown dataset: {cfg.run.dataset.name}\")\n        prompts.append(prompt)\n    \n    # [VALIDATOR FIX - Attempt 1]\n    # [PROBLEM]: 0% accuracy - model responses are misaligned with questions\n    # [CAUSE]: asyncio.as_completed() returns futures in completion order, not submission order,\n    #          causing responses to be paired with wrong examples\n    # [FIX]: Use asyncio.gather() to preserve order of results\n    #\n    # [OLD CODE]:\n    # results = []\n    # for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Querying API\"):\n    #     result = await f\n    #     results.append(result)\n    #\n    # [NEW CODE]:\n    # Run inference\n    print(f\"Running inference with {max_concurrent} concurrent requests...\")\n    tasks = [\n        query_openai_async(\n            client=client,\n            prompt=prompt,\n            model=cfg.run.model.name,\n            temperature=cfg.run.model.temperature,\n            max_tokens=cfg.run.model.max_tokens,\n            semaphore=semaphore\n        )\n        for prompt in prompts\n    ]\n    \n    # Use asyncio.gather to preserve order\n    results = []\n    with tqdm(total=len(tasks), desc=\"Querying API\") as pbar:\n        # Gather all results while maintaining order\n        results = await asyncio.gather(*tasks)\n        pbar.update(len(tasks))\n    \n    # Parse answers and compute accuracy\n    correct = 0\n    total = 0\n    unparseable = 0\n    \n    answer_format = cfg.run.inference.answer_format\n    parsed_results = []\n    \n    for i, (example, result) in enumerate(zip(dataset, results)):\n        response_text = result.get(\"response\", \"\")\n        gold_answer = example[\"gold_answer\"]\n        \n        if response_text is None:\n            parsed_answer = None\n            unparseable += 1\n        elif answer_format == \"numeric\":\n            parsed_answer = parse_numeric_answer(response_text)\n            if parsed_answer is None:\n                unparseable += 1\n        elif answer_format == \"multiple_choice\":\n            parsed_answer = parse_multiple_choice_answer(response_text)\n            if parsed_answer is None:\n                unparseable += 1\n        else:\n            raise ValueError(f\"Unknown answer format: {answer_format}\")\n        \n        is_correct = False\n        if parsed_answer is not None and gold_answer is not None:\n            # Normalize answers for comparison\n            if answer_format == \"numeric\":\n                try:\n                    is_correct = float(parsed_answer) == float(gold_answer)\n                except (ValueError, TypeError):\n                    is_correct = parsed_answer == gold_answer\n            else:\n                is_correct = parsed_answer == gold_answer\n            \n            if is_correct:\n                correct += 1\n            total += 1\n        \n        parsed_results.append({\n            \"example_id\": i,\n            \"question\": example[\"question\"],\n            \"gold_answer\": gold_answer,\n            \"model_response\": response_text,\n            \"parsed_answer\": parsed_answer,\n            \"is_correct\": is_correct,\n            \"usage\": result.get(\"usage\", {}),\n            \"error\": result.get(\"error\", None)\n        })\n    \n    accuracy = correct / total if total > 0 else 0.0\n    \n    # Log to WandB\n    if cfg.wandb.mode == \"online\":\n        wandb.log({\n            \"accuracy\": accuracy,\n            \"correct\": correct,\n            \"total\": total,\n            \"unparseable\": unparseable,\n            \"samples_processed\": len(dataset)\n        })\n        wandb.summary[\"final_accuracy\"] = accuracy\n        wandb.summary[\"correct\"] = correct\n        wandb.summary[\"total\"] = total\n        wandb.summary[\"unparseable\"] = unparseable\n    \n    # Save results\n    results_dir = Path(cfg.results_dir) / cfg.run.run_id\n    results_dir.mkdir(parents=True, exist_ok=True)\n    \n    with open(results_dir / \"results.json\", \"w\") as f:\n        json.dump(parsed_results, f, indent=2)\n    \n    with open(results_dir / \"metrics.json\", \"w\") as f:\n        json.dump({\n            \"accuracy\": accuracy,\n            \"correct\": correct,\n            \"total\": total,\n            \"unparseable\": unparseable,\n            \"samples_processed\": len(dataset)\n        }, f, indent=2)\n    \n    print(f\"\\nResults saved to {results_dir}\")\n    print(f\"Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    print(f\"Unparseable: {unparseable}\")\n    \n    # Sanity validation\n    if cfg.mode == \"sanity_check\":\n        perform_sanity_validation(parsed_results, accuracy, total)\n    \n    return accuracy\n\n\ndef perform_sanity_validation(results: List[Dict], accuracy: float, total: int):\n    \"\"\"Perform sanity validation checks for inference task.\"\"\"\n    samples_processed = len(results)\n    outputs_valid = sum(1 for r in results if r[\"model_response\"] is not None)\n    outputs_unique = len(set(r[\"model_response\"] for r in results if r[\"model_response\"] is not None))\n    \n    # Print summary\n    summary = {\n        \"samples\": samples_processed,\n        \"outputs_valid\": outputs_valid,\n        \"outputs_unique\": outputs_unique,\n        \"accuracy\": accuracy,\n        \"total\": total\n    }\n    print(f\"SANITY_VALIDATION_SUMMARY: {json.dumps(summary)}\")\n    \n    # Check validation conditions\n    if samples_processed < 5:\n        print(\"SANITY_VALIDATION: FAIL reason=insufficient_samples\")\n        sys.exit(1)\n    \n    if outputs_valid < 5:\n        print(\"SANITY_VALIDATION: FAIL reason=insufficient_valid_outputs\")\n        sys.exit(1)\n    \n    if outputs_unique < 2:\n        print(\"SANITY_VALIDATION: FAIL reason=all_outputs_identical\")\n        sys.exit(1)\n    \n    if total == 0:\n        print(\"SANITY_VALIDATION: FAIL reason=no_parseable_answers\")\n        sys.exit(1)\n    \n    print(\"SANITY_VALIDATION: PASS\")\n\n\ndef run_inference(cfg: DictConfig):\n    \"\"\"Main entry point for inference.\"\"\"\n    # Initialize WandB\n    if cfg.wandb.mode == \"online\":\n        # Override project for sanity check\n        project = cfg.wandb.project\n        if cfg.mode == \"sanity_check\":\n            project = f\"{project}-sanity\"\n        \n        run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=project,\n            name=cfg.run.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True)\n        )\n        print(f\"WandB run URL: {run.url}\")\n    \n    # Run inference\n    accuracy = asyncio.run(run_inference_async(cfg))\n    \n    if cfg.wandb.mode == \"online\":\n        wandb.finish()\n    \n    return accuracy\n\n\nif __name__ == \"__main__\":\n    # This script is invoked by main.py\n    # Config is passed via hydra\n    pass\n",
      "src/main.py": "\"\"\"Main orchestrator for running experiments.\"\"\"\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\nfrom pathlib import Path\n\nfrom src.inference import run_inference\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig):\n    \"\"\"Main orchestrator for a single run.\"\"\"\n    print(\"=\" * 80)\n    print(f\"Running experiment: {cfg.run.run_id}\")\n    print(f\"Mode: {cfg.mode}\")\n    print(\"=\" * 80)\n    print(\"\\nConfiguration:\")\n    print(OmegaConf.to_yaml(cfg))\n    print(\"=\" * 80)\n    \n    # Apply mode-specific overrides\n    if cfg.mode == \"sanity_check\":\n        # For inference tasks in sanity check mode:\n        # - Use fewer samples (already handled in inference.py)\n        # - Use wandb online mode to verify logging\n        # Override wandb project to avoid polluting main runs\n        if cfg.wandb.mode == \"online\":\n            cfg.wandb.project = f\"{cfg.wandb.project}-sanity\"\n    \n    # This is an inference-only task\n    # Run inference\n    accuracy = run_inference(cfg)\n    \n    print(\"=\" * 80)\n    print(f\"Experiment completed: {cfg.run.run_id}\")\n    print(f\"Final accuracy: {accuracy:.4f}\")\n    print(\"=\" * 80)\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "src/model.py": "\"\"\"Model definitions (not used for inference-only tasks).\"\"\"\n\n# This file is not used for the current inference-only experiment.\n# It would contain model definitions if training were required.\n\npass\n",
      "src/preprocess.py": "\"\"\"Dataset loading and preprocessing for GSM8K and ARC-Challenge.\"\"\"\n\nfrom datasets import load_dataset\nfrom typing import Dict, List, Any\nimport re\n\n\ndef load_gsm8k(split: str = \"test\", cache_dir: str = \".cache\") -> List[Dict[str, Any]]:\n    \"\"\"Load GSM8K dataset.\n    \n    Args:\n        split: Dataset split (train/test)\n        cache_dir: Directory to cache downloaded datasets\n        \n    Returns:\n        List of examples with 'question', 'answer', and 'gold_answer' keys\n    \"\"\"\n    dataset = load_dataset(\"gsm8k\", \"main\", split=split, cache_dir=cache_dir)\n    \n    examples = []\n    for item in dataset:\n        # Extract numeric answer from the format \"#### 123\"\n        answer_text = item[\"answer\"]\n        match = re.search(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\", answer_text)\n        gold_answer = match.group(1) if match else None\n        \n        examples.append({\n            \"question\": item[\"question\"],\n            \"answer\": answer_text,  # Full answer with reasoning\n            \"gold_answer\": gold_answer  # Numeric answer only\n        })\n    \n    return examples\n\n\ndef load_arc_challenge(split: str = \"test\", cache_dir: str = \".cache\") -> List[Dict[str, Any]]:\n    \"\"\"Load ARC-Challenge dataset.\n    \n    Args:\n        split: Dataset split (train/validation/test)\n        cache_dir: Directory to cache downloaded datasets\n        \n    Returns:\n        List of examples with 'question', 'choices', 'answer_key', and 'gold_answer' keys\n    \"\"\"\n    dataset = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=split, cache_dir=cache_dir)\n    \n    examples = []\n    for item in dataset:\n        choices = item[\"choices\"]\n        choice_text = \"\\n\".join([\n            f\"{label}: {text}\" \n            for label, text in zip(choices[\"label\"], choices[\"text\"])\n        ])\n        \n        examples.append({\n            \"question\": item[\"question\"],\n            \"choices\": choice_text,\n            \"choice_labels\": choices[\"label\"],\n            \"choice_texts\": choices[\"text\"],\n            \"answer_key\": item[\"answerKey\"],\n            \"gold_answer\": item[\"answerKey\"]  # The correct answer label (A/B/C/D)\n        })\n    \n    return examples\n\n\ndef get_dataset(name: str, split: str = \"test\", cache_dir: str = \".cache\") -> List[Dict[str, Any]]:\n    \"\"\"Load a dataset by name.\n    \n    Args:\n        name: Dataset name (gsm8k or arc-challenge)\n        split: Dataset split\n        cache_dir: Directory to cache downloaded datasets\n        \n    Returns:\n        List of dataset examples\n    \"\"\"\n    if name == \"gsm8k\":\n        return load_gsm8k(split=split, cache_dir=cache_dir)\n    elif name == \"arc-challenge\":\n        return load_arc_challenge(split=split, cache_dir=cache_dir)\n    else:\n        raise ValueError(f\"Unknown dataset: {name}\")\n",
      "src/train.py": "\"\"\"Training script (not used for inference-only tasks).\"\"\"\n\n# This file is not used for the current inference-only experiment.\n# It would contain training logic if training were required.\n\npass\n",
      "config/config.yaml": "# Main configuration for Hydra\n# This file will be populated by AI code generator based on experimental design\n\ndefaults:\n  - _self_\n  - run: ???  # loads from config/run/, accessible as 'run=' in CLI\n\n# Execution mode: main, sanity_check, or pilot\nmode: main\n\n# Results output directory\nresults_dir: .research/results\n\n# WandB configuration\nwandb:\n  entity: airas\n  project: 2026-0223-hypothesis\n  mode: online\n",
      "config/run/comparative-0-gpt4o-arc.yaml": "run_id: comparative-0-gpt4o-arc\n\nmethod:\n  type: comparative-0\n  name: standard-cot\n  prompt_template: |\n    Let's solve this multiple-choice science question step by step.\n    \n    Question: {question}\n    \n    Choices:\n    {choices}\n    \n    Think step-by-step and provide your final answer as: \"The answer is: [A/B/C/D]\"\n\nmodel:\n  provider: openai\n  name: gpt-4o\n  temperature: 0.0\n  max_tokens: 512\n\ndataset:\n  name: arc-challenge\n  split: test\n  cache_dir: .cache\n\ninference:\n  batch_size: 16\n  max_concurrent: 32\n  save_outputs: true\n  answer_format: multiple_choice\n",
      "config/run/comparative-0-gpt4o-gsm8k.yaml": "run_id: comparative-0-gpt4o-gsm8k\n\nmethod:\n  type: comparative-0\n  name: standard-cot\n  prompt_template: |\n    Let's solve this math problem step by step.\n    \n    Problem: {question}\n    \n    Think step-by-step and provide your final answer as: \"The answer is: [number]\"\n\nmodel:\n  provider: openai\n  name: gpt-4o\n  temperature: 0.0\n  max_tokens: 512\n\ndataset:\n  name: gsm8k\n  split: test\n  cache_dir: .cache\n\ninference:\n  batch_size: 16\n  max_concurrent: 32\n  save_outputs: true\n  answer_format: numeric\n",
      "config/run/proposed-gpt4o-arc.yaml": "run_id: proposed-gpt4o-arc\n\nmethod:\n  type: proposed\n  name: human-aligned-cot\n  prompt_template: |\n    You are solving a multiple-choice science question. Follow these cognitive phases:\n    \n    1. DECOMPOSE: Break down the question into key concepts and requirements.\n    2. ANALOGIZE: Think of related scientific principles or examples.\n    3. VERIFY: Check each answer choice against your reasoning.\n    4. FINAL: Select the best answer choice.\n    \n    Question: {question}\n    \n    Choices:\n    {choices}\n    \n    Think step-by-step and provide your final answer as: \"The answer is: [A/B/C/D]\"\n\nmodel:\n  provider: openai\n  name: gpt-4o\n  temperature: 0.0\n  max_tokens: 512\n\ndataset:\n  name: arc-challenge\n  split: test\n  cache_dir: .cache\n\ninference:\n  batch_size: 16\n  max_concurrent: 32\n  save_outputs: true\n  answer_format: multiple_choice\n",
      "config/run/proposed-gpt4o-gsm8k.yaml": "run_id: proposed-gpt4o-gsm8k\n\nmethod:\n  type: proposed\n  name: human-aligned-cot\n  prompt_template: |\n    You are solving a math problem. Follow these cognitive phases:\n    \n    1. DECOMPOSE: Break down the problem into smaller sub-problems.\n    2. ANALOGIZE: Think of similar problems or patterns you've seen.\n    3. VERIFY: Check each step for logical consistency.\n    4. FINAL: Provide the final numerical answer.\n    \n    Problem: {question}\n    \n    Think step-by-step and provide your final answer as: \"The answer is: [number]\"\n\nmodel:\n  provider: openai\n  name: gpt-4o\n  temperature: 0.0\n  max_tokens: 512\n\ndataset:\n  name: gsm8k\n  split: test\n  cache_dir: .cache\n\ninference:\n  batch_size: 16\n  max_concurrent: 32\n  save_outputs: true\n  answer_format: numeric\n",
      "pyproject.toml": "[project]\nname = \"hypothesis-test\"\nversion = \"0.1.0\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"hydra-core>=1.3.0\",\n    \"wandb>=0.16.0\",\n    \"openai>=1.0.0\",\n    \"datasets>=2.16.0\",\n    \"scipy>=1.11.0\",\n    \"matplotlib>=3.8.0\",\n    \"pandas>=2.1.0\",\n    \"numpy>=1.26.0\",\n    \"tqdm>=4.66.0\",\n    \"aiohttp>=3.9.0\",\n]\n"
    }
  }
}